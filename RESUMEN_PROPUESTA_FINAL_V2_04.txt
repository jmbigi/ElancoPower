RESUMEN PROPUESTA FINAL: Centralización de Datos y Analítica – Elanco Animal Health (Operación CASA)
Fecha: 10 de noviembre de 2025 | Versión: 2.04 (Con ABAP Developer - Cronograma Moderado) | Norma: < 1000 palabras
================================================================================

CONTEXTO Y PROBLEMA

El área de Planeamiento Financiero (FP&A) de Elanco requiere consultas manuales periódicas a SAP, lo que genera retrasos en reportes críticos, múltiples solicitudes a TI/Funcionales, inconsistencias en datos extraídos y ausencia de un Data Lake centralizado. Adicionalmente, existen antecedentes previos de carga en BigQuery (tickets 2020-2021) con errores estructurales (falta de índices, modelos no relacionales).

OBJETIVO DE LA PROPUESTA

Construir una arquitectura integral de Datos y Analítica que permita extraer, centralizar y procesar datos de SAP S/4HANA (módulos FI, CO, SD, MM), crear un Data Lake estructurado en Google BigQuery (capas RAW, PROCESSED, CURATED), desarrollar 12 dashboards operacionales en Power BI Service, implementar modelos predictivos de clasificación y regresión, y asegurar gobernanza mediante RLS (Row-Level Security) por países y áreas.

ALCANCE

La solución abarcará 18 transacciones SAP críticas distribuidas en cuatro módulos principales: FI (Financial Accounting) con 4 transacciones incluyendo FAGLL03, FB03, F.08 y F.01; CO (Controlling) con 2 transacciones KSB1 y KE24; SD (Sales & Distribution) con 2 transacciones VA05 y KE24; MM (Materials Management) con 6 transacciones que incluyen ME2L, ME23N, MM60, MB59, MB5B y MCHB; FI-AP/AR (Accounts Payable/Receivable) con 2 transacciones FBL1N y FBL5N; Master Data con 2 transacciones XK03 y XD03; y 2 transacciones custom críticas ZLEL008 y ZVEL015. Se trabajará con 32-38 tablas SAP incluyendo ACDOCA/ACDOCA_T como tabla universal de documentos contables (Universal Journal), BSEG, BKPF, EKKO, EKPO, VBAK, VBAP, entre otras.

PIPELINE TECNOLÓGICO

La arquitectura se basará en SAP SLT (Landscape Transformation Server) para replicación en tiempo real desde SAP S/4HANA hacia Google BigQuery. El Data Lake estará estructurado en tres capas: RAW (réplica directa), PROCESSED (transformaciones, limpieza, cálculo de KPIs) y CURATED (modelos analíticos listos para consumo). Power BI Service consumirá datos desde la capa CURATED mediante conectores nativos, con RLS configurada por país, centro de costo y área funcional. Se implementarán 12 dashboards operacionales con filtros dinámicos, requiriendo 8 licencias Power BI Pro renovables automáticamente. El modelo dimensional incluirá 8 dimensiones (Tiempo, País, Área, Cliente, Proveedor, Material, Cuenta, Centro de Costo) y 6 tablas de hechos (Ventas, Compras, Inventario, Contabilidad, CxC, CxP). La inclusión de un ABAP Developer dedicado permitirá la configuración avanzada de SAP SLT y el análisis profundo de las transacciones custom ZLEL008 y ZVEL015.

FASES DEL PROYECTO

El proyecto se estructura en tres fases principales. La Fase 0 de Revisión de Alcance y Factibilidad (6 semanas, 328 horas) incluye validación técnica de conectividad SAP SLT ↔ BigQuery, confirmación final de las 18 transacciones y tablas asociadas, análisis detallado de transacciones custom, definición de requerimientos de dashboards con usuarios clave, documentación de Data Governance y políticas RLS, desarrollo de POC técnico end-to-end, y reunión de Go/No-Go para aprobación de continuidad. La Fase 1 de Construcción del Data Lake (20 semanas, 852 horas) comprende configuración de SAP SLT para replicación en tiempo real, desarrollo de 18 pipelines de extracción y transformación organizados por módulo SAP, optimización de queries BigQuery y tuning de performance, automatización CI/CD y monitoreo de pipelines, validación de calidad de datos (DQM) y manejo de errores, y documentación técnica completa de la arquitectura implementada. La Fase 2 de Modelado y Dashboards (10 semanas, 700 horas) incluye diseño e implementación del modelo dimensional completo (8 dimensiones + 6 hechos), construcción de 12 dashboards operacionales en Power BI Service distribuidos en dashboards financieros, de ventas, de supply chain, y ejecutivos, implementación de RLS por país/área/centro de costo, pruebas UAT con usuarios finales en 4 fases (Financiero → Comercial → Supply → Ejecutivo), capacitación a usuarios clave (Power BI + autoservicio BigQuery), y ejecución del Go-Live.

ESFUERZO Y RECURSOS

El proyecto requiere un esfuerzo total de 1.880 horas distribuidas en 36 semanas con 4 perfiles especializados: Consultor BI (BigQuery, Power BI) con 935 horas (49.7%), ABAP Developer (Extracción SAP, SLT) con 270 horas (14.4%) participando solo en Fase 0 y Fase 1, Funcional SAP (Mapeo transacciones) con 512 horas (27.2%), y Project Manager (Coordinación, Governance) con 163 horas (8.7%). La distribución por fase es: Fase 0 (Alcance y Factibilidad) 328 horas (17.4%) en 6 semanas, Fase 1 (Construcción Data Lake) 852 horas (45.3%) en 20 semanas, y Fase 2 (Dashboards y Visualización) 700 horas (37.2%) en 10 semanas. El Go-Live estimado es el 13 de septiembre de 2026. La versión 2.04 incorpora un ABAP Developer dedicado (nuevo recurso respecto a V2.02) con experiencia de 5+ años en ABAP y 2+ años en SAP SLT, responsable de configuración y monitoreo de SAP SLT, extracción de datos SAP hacia BigQuery capa RAW, análisis de transacciones custom, y gestión de tickets SAP con TI Global, permitiendo así reducir el cronograma de 42 a 36 semanas (6 semanas menos, equivalente a 1 mes de Go-Live adelantado) con una inversión incremental de 290 horas respecto a V2.02.

ENTREGABLES

La propuesta incluye componentes funcionales (Data Lake en BigQuery con 3 capas operacionales, 18 pipelines SAP funcionando en producción, modelo dimensional con 8 dimensiones y 6 hechos, 12 dashboards en Power BI Service con RLS configurado, pipelines de extracción y transformación automatizados, monitoreo y alertas configurados), documentación técnica (arquitectura del Data Lake con diagramas y flujos de datos, documentación técnica de cada pipeline, diccionario de datos completo, manual de operación y mantenimiento, código fuente en repositorio Git), y capacitación y transferencia (sesiones para usuarios finales, documentación de gobierno de datos y políticas de acceso, documentación funcional de cada dashboard, transferencia de conocimiento a equipo TI de Elanco, documento de cierre de proyecto y lecciones aprendidas), más 3 meses de garantía post-Go-Live para soporte correctivo sin costo adicional.

BENEFICIOS

La implementación permitirá reducir en un 80% el tiempo de extracción manual de reportes SAP y los tickets asociados, centralizar datos en una única fuente de verdad (BigQuery) eliminando inconsistencias, automatizar reportes críticos con actualización diaria/semanal, democratizar el acceso a datos mediante Power BI Service (self-service analytics), establecer una base sólida para proyectos futuros de IA/ML y analítica avanzada, mejorar la toma de decisiones estratégicas mediante información oportuna y confiable, y lograr Go-Live 1 mes antes (13 de septiembre de 2026 vs mediados de octubre 2026 en V2.02), generando valor de negocio adelantado y eliminando aproximadamente 120 horas de operación manual en ese mes. El análisis costo-beneficio muestra que la inversión incremental de 290 horas se recupera en 2.4 meses post-Go-Live, considerando solo el ahorro operativo, y en 1-2 meses si se considera el valor de insights tempranos y reducción de riesgo técnico.

REQUISITOS

Los requisitos técnicos incluyen acceso técnico a SAP S/4HANA con permisos de lectura a 32-38 tablas, disponibilidad de SAP SLT (Landscape Transformation Server), proyecto Google Cloud con BigQuery activo, licencias de Power BI Service (8 Pro confirmadas), ancho de banda de red suficiente para replicación SAP SLT, y soporte de SAP Basis de Elanco para permisos y configuración SLT. Los requisitos organizacionales comprenden disponibilidad de stakeholders clave para workshops y UAT (FP&A, Finanzas, Comercial), aprobaciones de seguridad y compliance en plazos estimados, designación de un SPOC (Single Point of Contact) en Elanco, sesiones semanales de seguimiento con stakeholders, aprobación de presupuesto adicional (+290h vs V2.02), y accesos provistos por Elanco antes del kick-off (6 de enero de 2026). Los supuestos clave son: las 18 transacciones están activas y en uso regular, no hay restricciones legales/compliance para exportar datos a BigQuery, infraestructura SAP SLT está operativa o puede implementarse, no se requieren desarrollos ABAP complejos adicionales más allá de extracción estándar, volumetría de datos SAP no excede 500GB, permisos SAP se resuelven en Fase 0, ABAP Developer es contratado antes del kick-off, y stakeholders disponibles para workshops y UAT según cronograma.

RIESGOS

Los riesgos principales incluyen delay en contratación de ABAP Developer con lead time de 2-3 semanas (riesgo ALTO, mitigación: iniciar reclutamiento inmediatamente post-aprobación), latencia en aprobaciones de seguridad/compliance (mitigación: iniciar trámites en Fase 0 involucrando a InfoSec desde el arranque), tickets SAP sin resolver para permisos o tablas faltantes (riesgo MEDIO, mitigación: validación técnica intensiva en Fase 0 semanas 2-4), complejidad de transacciones custom ZLEL008 y ZVEL015 mayor a la estimada (riesgo MEDIO, mitigación: ABAP Developer dedicado con análisis profundo en Fase 0), calidad inconsistente de datos en SAP con duplicados o campos vacíos (mitigación: DQM robusto en capa PROCESSED con alertas automáticas de anomalías), disponibilidad limitada de SAP Basis de Elanco (riesgo BAJO, mitigación: plan de coordinación y calendario definido), indisponibilidad de usuarios clave para validaciones (mitigación: definir SPOC y backups, calendario de sesiones confirmado anticipadamente), cambios en alcance durante ejecución o scope creep (riesgo BAJO, mitigación: proceso formal de change request con documentación de impacto en cronograma/costo), y problemas de conectividad SAP SLT ↔ BigQuery (mitigación: validación técnica en Fase 0, soporte de BASIS SAP y equipo GCP).

EXCLUSIONES

El proyecto excluye mantenimiento de infraestructura SAP (responsabilidad de TI Elanco), licencias de SAP, BigQuery y Power BI (se asume provistas por cliente), desarrollos ABAP custom no relacionados con extracción estándar o nuevas transacciones, integración con sistemas no-SAP (CRM, ERP externos), migración de datos históricos previos al período definido, soporte correctivo más allá de 3 meses post-Go-Live (requiere contrato de mantenimiento separado), nuevos requerimientos o cambios de alcance no contemplados en esta propuesta (requieren change request formal), y rediseño de dashboards post-Go-Live (la garantía cubre solo ajustes menores).

CRITERIOS DE ÉXITO

El proyecto será exitoso cuando el Data Lake en BigQuery esté operativo con 3 capas (RAW, PROCESSED, CURATED), los 18 pipelines SAP estén funcionando en producción, el modelo dimensional esté completamente documentado (8 dimensiones + 6 hechos), los 12 dashboards en Power BI Service estén validados y en producción, el RLS esté configurado y verificado por país/área/centro de costo, la replicación SAP SLT funcione con latencia menor a 15 minutos, se reduzcan en 80% los tickets de extracción manual de reportes SAP, los usuarios finales estén capacitados y autónomos en consulta de dashboards y autoservicio BigQuery, la documentación técnica y funcional esté entregada y aprobada, el código fuente esté disponible en repositorio Git, el monitoreo y alertas estén configurados, y el Go-Live se ejecute exitosamente el 13 de septiembre de 2026.

PRÓXIMOS PASOS

Los próximos pasos son: revisión y aprobación de la propuesta versión 2.04 por parte de Elanco (FP&A, TI, Finanzas), decisión sobre inversión adicional (+290h = +18% presupuesto respecto a V2.02), confirmación de disponibilidad de recurso SAP Basis interno de Elanco, aprobación de cronograma de 36 semanas con Go-Live el 13 de septiembre de 2026, provisión de accesos SAP y BigQuery antes del kick-off, reclutamiento de ABAP Developer por parte de Aunergia (lead time 2-3 semanas), preparación de ambientes de desarrollo y testing, elaboración de plan detallado de Fase 0 semana a semana, kick-off meeting para alineamiento de expectativas y roles (6 de enero de 2026), inicio de Fase 0 con validación técnica de conectividad y alcance definitivo, definición de calendario de sesiones con usuarios clave (FP&A, Finanzas, Comercial), y firma de contrato y orden de compra para arranque formal del proyecto.



--------------------------------------------------------------------------------
Aunergia – Consultoría en Optimización de Procesos y TI
--------------------------------------------------------------------------------
